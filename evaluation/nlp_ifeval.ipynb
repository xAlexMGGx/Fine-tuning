{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install absl-py langdetect nltk immutabledict datasets bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36940d71c1274faaa15767a839ebe3d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   0%|          | 0/34 [00:00<?, ?batch/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "/home/usuario/.local/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:452: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "2024-11-25 12:03:35.059900: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-25 12:03:35.074148: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-25 12:03:35.081468: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-25 12:03:35.083684: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-25 12:03:35.090525: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-25 12:03:35.532787: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Processing Batches:   3%|▎         | 1/34 [02:07<1:10:14, 127.72s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:   6%|▌         | 2/34 [04:13<1:07:37, 126.81s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:   9%|▉         | 3/34 [06:19<1:05:15, 126.32s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  12%|█▏        | 4/34 [08:25<1:03:05, 126.19s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  15%|█▍        | 5/34 [10:32<1:01:03, 126.34s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  18%|█▊        | 6/34 [12:39<59:02, 126.54s/batch]  Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  21%|██        | 7/34 [14:47<57:11, 127.10s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  24%|██▎       | 8/34 [16:53<54:54, 126.73s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  26%|██▋       | 9/34 [18:59<52:46, 126.65s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  29%|██▉       | 10/34 [21:04<50:22, 125.93s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  32%|███▏      | 11/34 [23:10<48:18, 126.03s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  35%|███▌      | 12/34 [25:22<46:55, 127.97s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  38%|███▊      | 13/34 [27:28<44:32, 127.26s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  41%|████      | 14/34 [29:34<42:20, 127.00s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  44%|████▍     | 15/34 [31:41<40:09, 126.79s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  47%|████▋     | 16/34 [33:47<37:57, 126.53s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  50%|█████     | 17/34 [35:52<35:46, 126.26s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  53%|█████▎    | 18/34 [37:59<33:43, 126.47s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  56%|█████▌    | 19/34 [40:06<31:37, 126.49s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  59%|█████▉    | 20/34 [42:11<29:26, 126.15s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  62%|██████▏   | 21/34 [44:17<27:20, 126.16s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  65%|██████▍   | 22/34 [46:23<25:12, 126.03s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  68%|██████▊   | 23/34 [48:42<23:49, 129.94s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  71%|███████   | 24/34 [50:49<21:29, 128.96s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  74%|███████▎  | 25/34 [52:56<19:17, 128.59s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  76%|███████▋  | 26/34 [55:03<17:03, 127.99s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  79%|███████▉  | 27/34 [57:08<14:50, 127.20s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  82%|████████▏ | 28/34 [59:14<12:40, 126.80s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  85%|████████▌ | 29/34 [1:01:21<10:33, 126.74s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  88%|████████▊ | 30/34 [1:03:26<08:25, 126.35s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  91%|█████████ | 31/34 [1:05:32<06:18, 126.21s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  94%|█████████▍| 32/34 [1:07:38<04:12, 126.03s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches:  97%|█████████▋| 33/34 [1:09:44<02:05, 125.98s/batch]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing Batches: 100%|██████████| 34/34 [1:11:48<00:00, 126.71s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses saved to instruction_following_eval/data/input_response_data.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"google/IFEval\")\n",
    "\n",
    "# Model name\n",
    "# model_name = \"mistralai/Mistral-7B-v0.3\"\n",
    "model_name = \"../models/best_model\"\n",
    "\n",
    "# Load the tokenizer for Mistral\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    add_eos_token=True,\n",
    "    use_fast=True,\n",
    "    padding_side='left'\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to EOS token\n",
    "\n",
    "# Quantization configuration using bitsandbytes library\n",
    "compute_dtype = getattr(torch, \"float32\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load the pre-trained model with the specified quantization configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id  # Set the model's padding token ID\n",
    "\n",
    "# Disable gradients to save memory and computation\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)  # Disable gradient computation globally\n",
    "\n",
    "# Prepare the output file\n",
    "output_dir = \"instruction_following_eval/data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, \"input_response_data.jsonl\")\n",
    "\n",
    "# Batch processing\n",
    "batch_size = 16  # Adjust based on your GPU memory capacity\n",
    "max_length = 512  # Limit output length to avoid excessive memory usage\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(dataset['train']), batch_size), desc=\"Processing Batches\", unit=\"batch\"):\n",
    "        try:\n",
    "            if (i + batch_size) > len(dataset['train']):\n",
    "                batch_prompts = dataset['train']['prompt'][i:len(dataset['train'])]\n",
    "            else:\n",
    "                batch_prompts = dataset['train']['prompt'][i:i + batch_size]\n",
    "            # Tokenize inputs\n",
    "            inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            \n",
    "            # Generate responses\n",
    "            with torch.no_grad():  # Ensure gradients are disabled during generation\n",
    "                if max_length:\n",
    "                    outputs = model.generate(**inputs, max_new_tokens=max_length)\n",
    "                else:\n",
    "                    outputs = model.generate(**inputs)\n",
    "            # Decode responses\n",
    "            responses = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "            # Write each response directly to the file\n",
    "            for prompt, response in zip(batch_prompts, responses):\n",
    "                f.write(json.dumps({\"prompt\": prompt, \"response\": response}) + '\\n')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch {i}: {e}\")\n",
    "\n",
    "print(f\"Responses saved to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
