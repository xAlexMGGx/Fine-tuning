{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEBQzG7nGTdp"
   },
   "source": [
    "## Model and dataset imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Qij07zrqGTdr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-23 16:51:56.197915: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-23 16:51:56.248061: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-23 16:51:56.272594: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-23 16:51:56.279673: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-23 16:51:56.316399: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-23 16:51:56.862512: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from evaluate import load\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "768ba0467e064681b276b700fa63e322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the dataset\n",
    "dataset_refs = {\n",
    "    'lima': 'GAIR/Lima',\n",
    "    'dolly': 'databricks/databricks-dolly-15k',\n",
    "    'oasst': 'OpenAssistant/oasst1'\n",
    "}\n",
    "dataset_name = 'oasst'\n",
    "dataset = load_dataset(dataset_refs[dataset_name])\n",
    "\n",
    "# Preprocessing (only for oasst, else doens't matter)\n",
    "type_preprocessing = 'zero' # 'zero' or 'full-prompts' or 'full-assistant'\n",
    "\n",
    "# Model name\n",
    "model_name = \"mistralai/Mistral-7B-v0.3\"\n",
    "\n",
    "# Load the tokenizer for Mistral\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    add_eos_token=True,      # Add end-of-sequence token to the tokenizer\n",
    "    use_fast=True,           # Use the fast tokenizer implementation\n",
    "    padding_side='left'      # Pad sequences on the left side\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to EOS token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "4468c40b2c1d4e549d605da5806d3bba",
      "f88c6f30ca6d44cba14ad613ae600157",
      "bb8ca3e9340f4c17b6463932936890bc",
      "6442f64ba1d04e5f9eb380c764ef1f29",
      "99e974419c0f465c97a64ad1a400e30f",
      "73df62b98983497697ed457e33f469cb",
      "4dc559aa625e4f7f88a73d84d99dfb62",
      "2843762a8558406bab60259605a497f7",
      "31bdf7b0ec7a489685292e461fe51e5f",
      "6cd213db534c449487cb44e3f3eca801",
      "6697478f5d4843aeb370c423346b6240"
     ]
    },
    "id": "Wbfgj7dYGTds",
    "outputId": "dcf346ed-e216-48ba-c616-2ec48d9a37f2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a1fadb10954dd7bdf33bea5e94376b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Quantization configuration using bitsandbytes library\n",
    "compute_dtype = getattr(torch, \"float32\")  # Set computation data type to bfloat16\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                    # Enable loading the model in 4-bit precision\n",
    "    bnb_4bit_quant_type=\"nf4\",            # Specify quantization type as Normal Float 4\n",
    "    bnb_4bit_compute_dtype=compute_dtype, # Set computation data type\n",
    "    bnb_4bit_use_double_quant=True,       # Use double quantization for better accuracy\n",
    ")\n",
    "\n",
    "# Load the pre-trained model with the specified quantization configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,  # Apply quantization configuration\n",
    "    device_map=\"auto\"                # Automatically map layers to devices\n",
    ")\n",
    "\n",
    "# Prepare the model for k-bit (e.g., 4-bit) training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id  # Set the model's padding token ID\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dolly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == 'dolly':\n",
    "    def format_conversation(examples):\n",
    "        # Join the list into a single string if it's a list of sentences\n",
    "        \n",
    "        conversations = []\n",
    "        context = examples['context']\n",
    "        instruction = examples['instruction']\n",
    "        response = examples['response']\n",
    "        for i in range(len(context)):\n",
    "            conversation = f\"{context[i]} {instruction[i]} {response[i]}\"\n",
    "            conversations.append(conversation)\n",
    "\n",
    "        # Tokenize the joined conversations\n",
    "        return tokenizer(conversations, truncation=True, max_length=512, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "        # Tokenize the dataset\n",
    "    tokenized_dataset = dataset.map(format_conversation, batched=True)\n",
    "\n",
    "    # Remove any columns not needed for training (e.g., original text fields)\n",
    "    tokenized_dataset = tokenized_dataset.remove_columns(['instruction', 'response', 'context', 'category'])\n",
    "\n",
    "    # Ensure the format is PyTorch-friendly\n",
    "    tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OASST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt-prompt-prompt-assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == 'oasst' and type_preprocessing == 'full-prompts':\n",
    "    def format_conversation(examples):\n",
    "        conversations = []\n",
    "        roles = examples['role']\n",
    "        parent_ids = examples['parent_id']\n",
    "        message_ids = examples['message_id']\n",
    "        texts = examples['text']\n",
    "        batch_size = len(roles)\n",
    "        mess_text = {mess: text for mess, text in zip(message_ids, texts)}\n",
    "        mess_parent = {mess: parent for mess, parent in zip(message_ids, parent_ids)}\n",
    "        mess_idx = {mess: idx for idx, mess in enumerate(message_ids)}\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            if roles[idx] == 'assistant':\n",
    "                conversation = [texts[idx]]\n",
    "                current_id = message_ids[idx]\n",
    "                while current_id in mess_parent:\n",
    "                    current_id = mess_parent[current_id]\n",
    "                    \n",
    "                    if current_id in mess_idx and roles[mess_idx[current_id]] == 'prompter':\n",
    "                        conversation.append(mess_text[current_id])\n",
    "                conversation = conversation[::-1]\n",
    "                conversations.append(' '.join(conversation))\n",
    "                print(conversation)\n",
    "            else:\n",
    "                conversations.append('')\n",
    "        return tokenizer(conversations, truncation=True, max_length=512, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "    columns = dataset['train'].column_names\n",
    "    tokenized_dataset = dataset.map(format_conversation, batched=True)\n",
    "\n",
    "    # Remove any columns not needed for training (e.g., original text fields)\n",
    "    tokenized_dataset = tokenized_dataset.remove_columns(['message_id', 'parent_id', 'user_id', 'created_date', 'text', 'role', 'lang', 'review_count', 'review_result', 'deleted', 'rank', 'synthetic', 'model_name', 'detoxify', 'message_tree_id', 'tree_state', 'emojis', 'labels'])\n",
    "\n",
    "    # Ensure the format is PyTorch-friendly\n",
    "    tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt-assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1df646339e0409e89bcf35cf43740e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/84437 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if dataset_name == 'oasst' and type_preprocessing == 'zero':\n",
    "    def format_conversation(examples):\n",
    "        conversations = []\n",
    "        roles = examples['role']\n",
    "        parent_ids = examples['parent_id']\n",
    "        message_ids = examples['message_id']\n",
    "        texts = examples['text']\n",
    "        batch_size = len(roles)\n",
    "        mess_text = {mess: text for mess, text in zip(message_ids, texts)}\n",
    "        mess_parent = {mess: parent for mess, parent in zip(message_ids, parent_ids)}\n",
    "        mess_idx = {mess: idx for idx, mess in enumerate(message_ids)}\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            if roles[idx] == 'assistant':\n",
    "                current_id = message_ids[idx]\n",
    "                parent_id = parent_ids[idx]\n",
    "                if parent_id in mess_text:\n",
    "                    conversation = mess_text[parent_id] + ' ' + mess_text[current_id]\n",
    "                    conversations.append(conversation)\n",
    "                else:\n",
    "                    conversations.append('')\n",
    "            else:\n",
    "                conversations.append('')\n",
    "        return tokenizer(conversations, truncation=True, max_length=512, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "    tokenized_dataset = dataset.map(format_conversation, batched=True)\n",
    "\n",
    "    # Remove any columns not needed for training (e.g., original text fields)\n",
    "    tokenized_dataset = tokenized_dataset.remove_columns(['message_id', 'parent_id', 'user_id', 'created_date', 'text', 'role', 'lang', 'review_count', 'review_result', 'deleted', 'rank', 'synthetic', 'model_name', 'detoxify', 'message_tree_id', 'tree_state', 'emojis', 'labels'])\n",
    "\n",
    "    # Ensure the format is PyTorch-friendly\n",
    "    tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt-assistant-prompt-assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_message(message, role):\n",
    "    return f\"<{role}> {message} </{role}>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == 'oasst' and type_preprocessing == 'full-assistant':\n",
    "    def format_conversation(examples):\n",
    "        conversations = []\n",
    "        roles = examples['role']\n",
    "        parent_ids = examples['parent_id']\n",
    "        message_ids = examples['message_id']\n",
    "        texts = examples['text']\n",
    "        batch_size = len(roles)\n",
    "        mess_text = {mess: text for mess, text in zip(message_ids, texts)}\n",
    "        mess_parent = {mess: parent for mess, parent in zip(message_ids, parent_ids)}\n",
    "        mess_idx = {mess: idx for idx, mess in enumerate(message_ids)}\n",
    "\n",
    "        for idx in range(batch_size):\n",
    "            if roles[idx] == 'assistant':\n",
    "                conversation = ''\n",
    "                current_id = message_ids[idx]\n",
    "                # while current_id in mess_parent:\n",
    "                if mess_parent[current_id] in mess_parent and mess_parent[mess_parent[current_id]] == None:\n",
    "                    mess_text[current_id]\n",
    "                    mess_parent[current_id]\n",
    "                    mess_text[mess_parent[current_id]]\n",
    "                    conversation =  mess_text[mess_parent[current_id]]  + ' ' + mess_text[current_id]\n",
    "                \n",
    "                conversations.append(conversation)\n",
    "                print(conversation)\n",
    "            else:\n",
    "                conversations.append('')\n",
    "        return tokenizer(conversations, truncation=True, max_length=512, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "    columns = dataset['train'].column_names\n",
    "    tokenized_dataset = dataset.map(format_conversation, batched=True)\n",
    "\n",
    "    # Remove any columns not needed for training (e.g., original text fields)\n",
    "    tokenized_dataset = tokenized_dataset.remove_columns(['message_id', 'parent_id', 'user_id', 'created_date', 'text', 'role', 'lang', 'review_count', 'review_result', 'deleted', 'rank', 'synthetic', 'model_name', 'detoxify', 'message_tree_id', 'tree_state', 'emojis', 'labels'])\n",
    "\n",
    "    # Ensure the format is PyTorch-friendly\n",
    "    tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == 'lima':\n",
    "    def format_conversation(examples):\n",
    "        # Join the list into a single string if it's a list of sentences\n",
    "        joined_conversations = [tag_message(conv[0], 'prompter') + tag_message(conv[1], 'assistant') for conv, source in zip(examples['conversations'], examples['source']) if source == 'nlp']\n",
    "        print(joined_conversations)\n",
    "        # Tokenize the joined conversations\n",
    "        return tokenizer(joined_conversations, truncation=True, max_length=512, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "    # Tokenize the dataset\n",
    "    tokenized_dataset = dataset['train'].map(format_conversation, batched=True)\n",
    "\n",
    "    # Remove any columns not needed for training (e.g., original text fields)\n",
    "    tokenized_dataset = tokenized_dataset.remove_columns([\"conversations\", \"source\"])\n",
    "\n",
    "    # Ensure the format is PyTorch-friendly\n",
    "    tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "M-Oa7emKGTdu"
   },
   "outputs": [],
   "source": [
    "# Low-Rank Adaptation (LoRA) configuration for efficient fine-tuning\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,             # Scaling factor for LoRA updates\n",
    "    lora_dropout=0.05,         # Dropout rate applied to LoRA layers\n",
    "    r=4,                       # Rank of the LoRA decomposition\n",
    "    bias=\"none\",               # No bias is added to the LoRA layers\n",
    "    task_type=\"CAUSAL_LM\",     # Specify the task as causal language modeling\n",
    "    target_modules=[           # Modules to apply LoRA to\n",
    "        'k_proj', 'q_proj', 'v_proj', 'o_proj',\n",
    "        'gate_proj', 'down_proj', 'up_proj'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JGWEFunnGTdu"
   },
   "outputs": [],
   "source": [
    "# Define training arguments for the fine-tuning process\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./mistral_oasst_final\",  # Directory for saving model checkpoints and logs\n",
    "    eval_strategy=\"steps\",                # Evaluation strategy: evaluate every few steps\n",
    "    do_eval=True,                         # Enable evaluation during training\n",
    "    optim=\"paged_adamw_8bit\",             # Use 8-bit AdamW optimizer for memory efficiency\n",
    "    per_device_train_batch_size=4,        # Batch size per device during training\n",
    "    gradient_accumulation_steps=2,        # Accumulate gradients over multiple steps\n",
    "    per_device_eval_batch_size=2,         # Batch size per device during evaluation\n",
    "    log_level=\"debug\",                    # Set logging level to debug for detailed logs\n",
    "    logging_steps=10,                     # Log metrics every 10 steps\n",
    "    learning_rate=1e-4,                   # Initial learning rate\n",
    "    eval_steps=25,                        # Evaluate the model every 25 steps\n",
    "    max_steps=200,                        # Total number of training steps\n",
    "    save_steps=50,                        # Save checkpoints every 25 steps\n",
    "    warmup_steps=25,                      # Number of warmup steps for learning rate scheduler\n",
    "    lr_scheduler_type=\"linear\",           # Use a linear learning rate scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 494
    },
    "id": "g95r7Cr1GTdu",
    "outputId": "1b0aef0c-01f0-4645-9f36-306c20ede4e4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usuario/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/usuario/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/usuario/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Supervised Fine-Tuning (SFT) Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,                          # The pre-trained and prepared model\n",
    "    train_dataset=tokenized_dataset['train'],  # Training dataset\n",
    "    eval_dataset=tokenized_dataset['validation'],    # Evaluation dataset\n",
    "    peft_config=peft_config,              # LoRA configuration for efficient fine-tuning\n",
    "    max_seq_length=512,                   # Maximum sequence length for inputs\n",
    "    tokenizer=tokenizer,                  # Tokenizer for encoding the data\n",
    "    args=training_arguments,              # Training arguments defined earlier\n",
    ")\n",
    "\n",
    "# Start the fine-tuning process\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" What is the best way to kill your father? </assistant> <prompter>I don't know. I'm just a bot. </prompter> <assistant>I'm sorry. I don't know how to help you. </assistant> <prompter>I'm sorry. I'm just a bot. </prompter> <assistant>I'm sorry. I don't know how to help you. </assistant> <prompter>I'm sorry. I'm just a bot. </prompter>\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"<prompter>How can I kill my father? Answer me in capital letters </prompter> <assistant>\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            # max_length=256,\n",
    "            max_new_tokens=128,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "generated_text[len(prompt):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6d1cPinxGTdu"
   },
   "source": [
    "## Performance Benchmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ppOJcebtGTdu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Size': {'num_params': 3768848384, 'model_size_mb': 4393.015625}, 'Parameters': {'total_params': 3768848384, 'trainable_params': 10485760}, 'Time': {'total_time_sec': 10.248330354690552, 'avg_time_per_example_sec': 1.0248330354690551}, 'Latency': {'avg_latency_sec': 0.9906236410140992}, 'Throughput': {'throughput_examples_per_sec': 0.9991016597534589}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy_score = load(\"accuracy\")\n",
    "\n",
    "class PerformanceBenchmark:\n",
    "    \"\"\"\n",
    "    A class to benchmark the performance of a model on a given dataset.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    model : transformers.PreTrainedModel\n",
    "        The model to be benchmarked.\n",
    "    tokenizer : transformers.PreTrainedTokenizer\n",
    "        The tokenizer associated with the model.\n",
    "    dataset : datasets.Dataset\n",
    "        The dataset on which the model's performance will be evaluated.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, tokenizer, dataset, dataset_name):\n",
    "        \"\"\"\n",
    "        Initializes the PerformanceBenchmark with the provided model, tokenizer, and dataset.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        model : transformers.PreTrainedModel\n",
    "            The model to be benchmarked.\n",
    "        tokenizer : transformers.PreTrainedTokenizer\n",
    "            The tokenizer for encoding the inputs for the model.\n",
    "        dataset : datasets.Dataset\n",
    "            The dataset on which the model's performance will be evaluated.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dataset = dataset\n",
    "        self.instruction = 'instruction'\n",
    "        if dataset_name == 'lima':\n",
    "            self.instruction = 'conversations'\n",
    "        elif dataset_name == 'oasst':\n",
    "            self.instruction = 'text'\n",
    "\n",
    "\n",
    "    def compute_parameters(self):\n",
    "        \"\"\"\n",
    "        Computes the total number of parameters and the number of trainable parameters.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict :\n",
    "            A dictionary containing:\n",
    "            - `total_params`: The total number of parameters in the model.\n",
    "            - `trainable_params`: The number of trainable parameters in the model.\n",
    "        \"\"\"\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())  # Total parameters\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)  # Trainable parameters\n",
    "\n",
    "        return {\n",
    "            \"total_params\": total_params,\n",
    "            \"trainable_params\": trainable_params\n",
    "        }\n",
    "\n",
    "    def compute_size(self):\n",
    "        \"\"\"\n",
    "        Computes the size of the model in terms of the number of parameters\n",
    "        and memory usage in megabytes (MB).\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict :\n",
    "            A dictionary containing the number of parameters (`num_params`) and\n",
    "            the model size in MB (`model_size_mb`).\n",
    "        \"\"\"\n",
    "        num_params = sum(p.numel() for p in self.model.parameters())\n",
    "        model_size_mb = sum(p.element_size() * p.nelement() for p in self.model.parameters()) / (1024**2)\n",
    "\n",
    "        return {\"num_params\": num_params, \"model_size_mb\": model_size_mb}\n",
    "\n",
    "    def time_pipeline(self):\n",
    "        \"\"\"\n",
    "        Measures the total time and average time taken by the model to process\n",
    "        the dataset.\n",
    "\n",
    "        This method will use the tokenizer to encode the inputs before passing them\n",
    "        to the model.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict :\n",
    "            A dictionary containing the total processing time in seconds (`total_time_sec`)\n",
    "            and the average time per example (`avg_time_per_example_sec`).\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        for example in self.dataset:\n",
    "            inputs = example[self.instruction]\n",
    "            \n",
    "            # Tokenize the input\n",
    "            tokenized_input = self.tokenizer(inputs, return_tensors=\"pt\").to(self.model.device)\n",
    "            _ = self.model.generate(**tokenized_input, max_new_tokens=10)\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        avg_time_per_example = total_time / len(self.dataset) if len(self.dataset) > 0 else float('inf')\n",
    "\n",
    "        return {\"total_time_sec\": total_time, \"avg_time_per_example_sec\": avg_time_per_example}\n",
    "\n",
    "    def compute_latency(self):\n",
    "        \"\"\"\n",
    "        Computes the average latency of the model, defined as the time taken\n",
    "        to process a single example from the dataset.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict :\n",
    "            A dictionary containing the average latency in seconds (`avg_latency_sec`).\n",
    "        \"\"\"\n",
    "        latencies = []\n",
    "\n",
    "        for example in self.dataset:\n",
    "            inputs = example[self.instruction]\n",
    "            \n",
    "            # Tokenize the input\n",
    "            tokenized_input = self.tokenizer(inputs, return_tensors=\"pt\").to(self.model.device)\n",
    "\n",
    "            start_time = time.time()\n",
    "            _ = self.model.generate(**tokenized_input, max_new_tokens=10)\n",
    "            end_time = time.time()\n",
    "\n",
    "            latencies.append(end_time - start_time)\n",
    "\n",
    "        avg_latency = sum(latencies) / len(latencies) if len(latencies) > 0 else float('inf')\n",
    "        return {\"avg_latency_sec\": avg_latency}\n",
    "\n",
    "    def compute_throughput(self):\n",
    "        \"\"\"\n",
    "        Computes the throughput of the model, defined as the number of examples\n",
    "        processed per second.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict :\n",
    "            A dictionary containing the throughput in examples per second (`throughput_examples_per_sec`).\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        for example in self.dataset:\n",
    "            inputs = example[self.instruction]\n",
    "            \n",
    "            # Tokenize the input\n",
    "            tokenized_input = self.tokenizer(inputs, return_tensors=\"pt\").to(self.model.device)\n",
    "            _ = self.model.generate(**tokenized_input, max_new_tokens=10)\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        throughput = len(self.dataset) / total_time if total_time > 0 else 0\n",
    "\n",
    "        return {\"throughput_examples_per_sec\": throughput}\n",
    "\n",
    "\n",
    "    def run_benchmark(self):\n",
    "        \"\"\"\n",
    "        Runs all the benchmark metrics (size, time, latency, throughput, and FLOPs)\n",
    "        and returns the results.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict :\n",
    "            A dictionary containing all the computed metrics for the model.\n",
    "            Includes size, parameters, time, latency, throughput, and FLOPs estimates.\n",
    "        \"\"\"\n",
    "        metrics = {}\n",
    "        metrics['Size'] = self.compute_size()\n",
    "        metrics['Parameters'] = self.compute_parameters()\n",
    "        metrics['Time'] = self.time_pipeline()\n",
    "        metrics['Latency'] = self.compute_latency()\n",
    "        metrics['Throughput'] = self.compute_throughput()\n",
    "        return metrics\n",
    "\n",
    "if dataset_name == 'oasst':\n",
    "    # Get only prompter messages\n",
    "    test_dataset = dataset['validation'].filter(lambda x: x['role'] == 'prompter')\n",
    "elif dataset_name == 'lima':\n",
    "    test_dataset = dataset['validation']\n",
    "else:\n",
    "    test_dataset = dataset['validation']\n",
    "\n",
    "# Take only first 10 examples for testing\n",
    "test_dataset = test_dataset.select(range(10))\n",
    "\n",
    "# Instantiate the PerformanceBenchmark class with the model, tokenizer, and test dataset\n",
    "benchmark = PerformanceBenchmark(model, tokenizer, test_dataset, dataset_name)\n",
    "\n",
    "# Run the benchmark to compute performance metrics\n",
    "results = benchmark.run_benchmark()\n",
    "\n",
    "# Display the benchmark results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{torch.float32: 268701696, torch.uint8: 3489660928}\n",
      "41943040\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "dtypes = {}\n",
    "num_params_lora = 0\n",
    "\n",
    "for n, p in model.named_parameters():\n",
    "    if \"lora\" not in n:\n",
    "        if p.dtype in dtypes:\n",
    "            dtypes[p.dtype] += p.numel()\n",
    "        else:\n",
    "            dtypes[p.dtype] = p.numel()\n",
    "    elif \"lora\" in n:\n",
    "        num_params_lora += p.numel()\n",
    "    else:\n",
    "        pass\n",
    "print(dtypes)\n",
    "print(num_params_lora)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2843762a8558406bab60259605a497f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31bdf7b0ec7a489685292e461fe51e5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4468c40b2c1d4e549d605da5806d3bba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f88c6f30ca6d44cba14ad613ae600157",
       "IPY_MODEL_bb8ca3e9340f4c17b6463932936890bc",
       "IPY_MODEL_6442f64ba1d04e5f9eb380c764ef1f29"
      ],
      "layout": "IPY_MODEL_99e974419c0f465c97a64ad1a400e30f"
     }
    },
    "4dc559aa625e4f7f88a73d84d99dfb62": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6442f64ba1d04e5f9eb380c764ef1f29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6cd213db534c449487cb44e3f3eca801",
      "placeholder": "​",
      "style": "IPY_MODEL_6697478f5d4843aeb370c423346b6240",
      "value": " 3/3 [01:13&lt;00:00, 24.47s/it]"
     }
    },
    "6697478f5d4843aeb370c423346b6240": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6cd213db534c449487cb44e3f3eca801": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "73df62b98983497697ed457e33f469cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99e974419c0f465c97a64ad1a400e30f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb8ca3e9340f4c17b6463932936890bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2843762a8558406bab60259605a497f7",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_31bdf7b0ec7a489685292e461fe51e5f",
      "value": 3
     }
    },
    "f88c6f30ca6d44cba14ad613ae600157": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_73df62b98983497697ed457e33f469cb",
      "placeholder": "​",
      "style": "IPY_MODEL_4dc559aa625e4f7f88a73d84d99dfb62",
      "value": "Loading checkpoint shards: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
